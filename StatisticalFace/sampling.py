# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_sampling.ipynb.

# %% auto 0
__all__ = ['bootstrapping', 'confidence_intervals_large_samples', 'standard_error']

# %% ../nbs/00_sampling.ipynb 3
import logging
logging.basicConfig(
    filename="logger_sampling.txt",
    filemode='a',
    format='%(asctime)s : %(levelname)s : %(message)s', 
    level=logging.INFO
    )

# %% ../nbs/00_sampling.ipynb 4
import numpy as np
from statistics import NormalDist

# %% ../nbs/00_sampling.ipynb 6
def bootstrapping( np_data, np_func, size, flag_clean_nan = False ):
    """
    @size: number of bootstrapping samples
    @np_funct: numpy function for reducing the samples (e.g., median, mean, max)
    @flag_clean_nan: flag to eliminate Nan values in the np tensor
    """
    #Cleaning NaNs
    if flag_clean_nan:
        np_data = np_data[ np.logical_not( np.isnan(np_data) ) ] 
    
    #Creating the boostrap replicates as long as the original data size
    #This strategy might work as imputation 
    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]
    
    logging.info("Empirical Estimate: " + str(np_func( np_data )) ) #Empirical Mean,Median,Max, etc
    logging.info("Bootstrapped Estimate: " + str( np_func( bootstrap_repl ) ) ) #Bootstrapped Mean,Median,Max, etc
    
    return np.array( bootstrap_repl )

# %% ../nbs/00_sampling.ipynb 8
def confidence_intervals_large_samples(data, confidence=0.95):
    """
    @confidence: confidence interval 
    @return: tuple (lowerbound, uperbound, h-value)
    """
    dist = NormalDist.from_samples( data )
    z = NormalDist().inv_cdf((1 + confidence) / 2.)
    h = dist.stdev * z / ((len(data) - 1) ** .5)
    return dist.mean - h, dist.mean + h, h

# %% ../nbs/00_sampling.ipynb 10
def standard_error(bootstrapped_data):
    return np.std( bootstrapped_data )
